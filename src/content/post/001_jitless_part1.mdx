---
title: "Bootstrapping an AOT Interpreter for SpiderMonkey"
publishDate: 2026-02-10
description: "Patching Pointers to Supply a Runtime Generated Interpreter at Compile Time."
tags: ["cpp", "jit", "spidermonkey"]
---

import BlogImage from '../../components/BlogImage.astro';


## Why a Generated Interpreter?

You may have heard that high performance interpreters are commonly
written in assembly. Some of the advantages of doing so include
attaining [fine grained control
flow](https://en.wikipedia.org/wiki/Threaded_code), [predictable
branching patterns](https://llvm.org/docs/BranchWeightMetadata.html),
[manual code
layout](https://sillycross.github.io/2023/05/12/2023-05-12/), and
escaping the C calling convention. SpiderMonkey's baseline interpreter
is no exception. What may surprise you, however, is that
SpiderMonkey's Baseline Interpreter is _generated at runtime_.

What does it mean for the interpreter to be _generated_? The
simplified picture is that, similar to any other JIT code in the
engine, we allocate a buffer on the heap, write some instructions into
it, memory map it to executable, then jump into it.

The more important question to ask is, _why would an interpreter be
generated?_ Sure, JIT compilers produce runtime generated code, but
that is necessitated by dynamic type information. An interpreter has
deterministic structure, doesn't it? Why should it not be a regular
compiled object available at link time? To understand, we'll take a
dive into some SpiderMonkey internals. We will see how runtime
generation is a natural strategy to take, how it yields some slight
performance benefits, and what the challenges are for providing it
AOT.


This work is part of an ongoing project to improve [Jitless mode]()
for SpiderMonkey. In this post, we introduce the bootstrapping
infrastructure for the AOT baseline interpreter, which involves
dumping runtime generated code, rebuilding and patching at load time.
In later posts, we may talk about extending this infrastructure to
trampolines, AOT Inline Cache stubs and other code.


## Macro Assemblers

Assembly allows a programmer to chisel out the intracite control flow
rquired for an interpreter. But actually writing one in pure assembly
is a grueling task. Instead, developers use high level languages, like
C++ in our case, to emit assembly for them. In SpiderMonkey, this
interface is called the
[MacroAssembler](https://searchfox.org/firefox-main/rev/133582f487ff8291ec10bd524db52db0b8ed363e/js/src/jit/MacroAssembler.h#).
Analogous interfaces exist in other engines, such as
[DynASM](https://luajit.org/dynasm.html) in LuaJIT and V8's
[MacroAssembler](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/codegen/macro-assembler.h).

```c++
// A simple MacroAssembler interface.
class MacroAssembler {
  public:
    void load(Register dst, Register src);
    void store(Register dst, Register src);
    void add(Register src1, Register src2, Register dst);
    void jmp(Register dst);
};
```

Using such an interface provides more than just developer ergonomics.
It also facilitates a much needed layer of abstraction between
describing the computation and the architectural specifics. While no
abstraction is perfectly leak-free (see some [quirks with ARM]() for
example), overall, a `load`, `store` or `add` are perfectly reasonable
operations to abstract.

Second, using a macro-assembler makes the code more self-explanatory
than raw assembly. We can design reusable compound routines like
[emitStackFrame]() or [callVM]() that are built from primitives. These
lower directly into the repetitive instruction sequences we would have
had to write painstakingly one at a time.

Last, and crucially, so perhaps not least, we only need to maintain
the backends responsible for correctly lowering the program, _not the
program itself_. Maintaining parallel interpreter implementations is
quite difficult, as V8 learned while [balancing several
implementations at once]().

## Generated Interpreter

Now we understand why interpreters are not written in assembly
directly. Returning to answer the question of why the interpreter is
generated - one plain reason is that the MacroAssembler itself is only
available at runtime. It is part of the same build artifact as the
engine, so how would we use it before it is compiled? Such chicken egg
dependencies can be solved with [bootstrapping]() in general. However,
if this were the only reason, the process to provide the baseline
interpreter AOT would be quite simple. We simply generate the
interpreter, dump it to a file, attach it as a build input and
rebuild. That sounds too nice. And in fact it is.

<BlogImage src="/images/jitless_part1/bootstrap.svg" alt="AOT blob
structure showing code and patches" darkMode="invert" />

Since the interpreter is generated relatively late in the startup
lifecycle, the clever engineers at Mozilla noticed that they could
bake in addresses of runtime objects directly into the interpreter
code, such as [opcode handlers](), [profiling]() and [debugging]()
infrastructure, and, most importantly, the addresses in the [dispatch
table](). Baking in runtime addresses is great for performance. We
save the pointer dereferences otherwise necessary to access fields of
our runtime objects, and the branch predictor benefits from seeing
stable target addresses rather than values loaded through indirection.
This is the performance convenience I mentioned earlier; it is also
the very reason for which bootstrapping the interpreter is difficult.

## ASLR and the Scope of the Problem

[ASLR](https://en.wikipedia.org/wiki/Address_space_layout_randomization)
randomizes the location of every heap allocation, every `mmap`'d
segment, and the main executable on each launch. A pre-compiled blob
with embedded absolute addresses would dereference garbage pointers on
any subsequent run. Therefore we need to keep track of every single
one.

Our AOT `JitCode` lives in the `AtomsZone` for immortal objects,
giving it a stable address throughout execution, but not a consistent
one _between_ executions. Techniques like [heap snapshotting]() or
[static roots]() could provide that, and are a potential direction for
the future. For now, we have to find and fix every runtime pointer
being emitted. In a typical AOT blob, the total patch count lands
around 300-400 entries, falling into natural categories:

- Dispatch table (256 entries): one per bytecode opcode. Each points
  to a handler _within the blob itself_, so it depends on where the
  blob was loaded in memory.

- Runtime data pointers (~10 kinds): `JSContext*`, `JitRuntime*`,
  `Realm*`, `wellKnownSymbols`, GC write barrier state, and profiler
  flags. These resolve to heap objects that move on every launch.

- External code pointers: VM wrapper trampolines, debugger trap
  handlers, and a handful of direct C++ function pointers.

## Patching and Indirection



### Pointer Patching

There are two main strategies we can take to fix our baked in runtime
pointer problem. The first is pointer patching. Suppose we augment the
interpreter codegen to watch for every time we emit an absolute
address. When we do so, a `RuntimePatch` is created, a small struct
which records the current offset, the kind of pointer to patch, and an
additional payload if necessary. An array of `RuntimePatch` structs
are serialized to the bottom of our AOT blob.

We are then set up to bootstrap. In a second build, we disassemble the
code portion of the blob into a `.s`, which can be registered as a
build input. At load time, the metadata portion of the blob is
deserialized, including the patches, which can be applied in the
correct heap and runtime context. Below is a slightly simplified
interface for emitting patches.

```c++
/// A simplified RuntimePatch structure. We track a variant for
/// every type of runtime pointer we encounter. The PatchContext
/// is provided at load-time to attain the correct values for the
/// runtime pointers.
class RuntimePatch {
  public:
    enum class Kind : uint16_t {
      JitRuntime,
      DispatchTable,
      VMWrapper,
      JitActivation,
      RealmPtr,
      ProfilerEnabled,
      DebugTrapHandler
    };

    Kind kind;
    uint32_t targetOffset;
    union {
      uint32_t handlerOffset;
      VMFunctionId vmId;
      DebugTrapHandlerKind dbgKind;
    };
    explicit RuntimePatch(Kind kind_, uint32_t targetOffset_) :
      kind(kind_), targetOffset(targetOffset_) {}

    void apply(const PatchContext& pc) const;
  private:
    RuntimePatch() = default;
    uintptr_t _getValueToPatch(const PatchContext& pc) const;
};
```

Finding the exhaustive locations where raw runtime pointers were
emitted in baseline codegen was tedious. Some are obvious and occur in
the top level logic, which we could find by grepping. Others were
obfuscated through layers of abstraction. Particularly tricky are
those emitted in routines shared by other codegen machinery. We can't
automatically apply a patch in these paths since they are shared. As a
result, lots of the patch logic used the following pattern, using a
macro and internal condition to emit the patch selectively.

```c++
#ifdef ENABLE_AOT_BASELINE
    if (isAOTCompile_) {
        emitPatchableMovImm(RuntimePatch::Kind::WellKnownSymbols, scratch2);
    } else
#endif
    {
      // The JIT generated case.
      masm.movePtr(ImmPtr(&runtime->wellKnownSymbols()), scratch2);
    }
```

Where the `emitPatchableMovImm` creates the `RuntimePatch` and
registers it:

```c++
void emitPatchableMovImm(RuntimePatch::Kind kind, Register dest) {
    CodeOffset off = masm.movWithPatch(ImmPtr((void*)AOT_PATCH_SENTINEL), dest);
    uint32_t immOff = off.offset() - sizeof(void*);
    aotAccumulator_.registerPatch(RuntimePatch(kind, immOff));
}
```

The codegen above happens to only be called from baseline interpreter
codegen. But we encountered many runtime pointers which were emitted
in nested calls into the MacroAssembler. As the central codegen class,
`masm` does a lot of lifting for the codegen in SpiderMonkey, with
[various external projects]() depending on the API it exposes. For
that reason, cluttering our patch accumulator into the scope of the
masm was not a viable solution. But if the masm must remain as
uncoupled as possible from baseline codegen, how do we patch runtime
pointers that are emitted in shared masm routines?

### Indirection Through a Pinned Register

For deeply nested routines we need a second strategy, a solution as
old as programming itself - indirection. We can't couple our patch
infrastructure into the MacroAssembler, but we can introduce a lighter
weight `isAOTCompile_` flag to emit conditional codegen.

When the flag is set, rather than emitting a raw pointer to the
[JSRuntime]() or [JSContext](), we load it from a guaranteed location
instead. We can even pin this pointer in a register so it is always
available, making the indirections slightly less costly. For baseline
interpreter generation, we pass a `JSZone` pointer in the
[BaselineFrame]() which facilitates the transition between host C++
code and the JIT compiled baseline interpreter. Immediately in the
prologue we load the zone pointer into a register<sup>1</sup>, then
compute the runtime pointers through indirection on this
register<sup>2</sup>.

### The Extended Jump Table

Some of the trickier runtime addresses were being leaked from the
extended jump table. The dispatch table patches were working on a vast
majority of tests, but once in a while stale jumps were appearing. It
turns out, when `call(ImmPtr(target))` can't reach the target with a
32-bit relative offset, the assembler sneakily creates a trampoline at
the bottom of the interpreter blob. We had to patch these too,
rewriting the `callWithABI(ImmPtr(fn))` with a patchable `movabs` into
a register followed by `callWithABI(reg)`.

```asm
;; Extended jump table entry:
movabs $0x7f1234567890, %r11
jmp    *%r11
```

Debug instrumentation like `assumeUnreachable()` internally calls
`callWithABI` too, creating hidden entries. We suppress these with `if
(!isAOTCompile_)`.

### Crash at Dumptime

With hundreds of patch sites, it is easy to miss one. As a safety net,
we added a verification pass that records every `movabs` whose
immediate looks like a pointer<sup>3</sup> and cross-checks it against
the `RuntimePatch` list after codegen. Any unmatched pointer is a
fatal error. Without this, a single forgotten address would produce a
blob that works on the build machine but crashes on every other run.

## Emit an Opcode

Let's put it all together with a concrete example. Here is the
emission of the `Symbol` opcode, which loads a well known symbol such
as `Symbol.iterator` or `Symbol.toStringTag`.

```c++
template <>
bool BaselineInterpreterCodeGen::emit_Symbol() {
  Register scratch1 = R0.scratchReg();
  Register scratch2 = R1.scratchReg();
  LoadUint8Operand(masm, scratch1);

#if defined(ENABLE_JS_AOT_ICS) || defined(ENABLE_AOT_BASELINE)
  if (isAOTCompile_) {
    emitPatchableMovImm(RuntimePatch::Kind::WellKnownSymbols, scratch2);
  } else
#endif
  {
    masm.movePtr(ImmPtr(&runtime->wellKnownSymbols()), scratch2);
  }
  masm.loadPtr(BaseIndex(scratch2, scratch1, ScalePointer), scratch1);
  masm.tagValue(JSVAL_TYPE_SYMBOL, scratch1, R0);
  frame.push(R0);
  return true;
}
```

In the generated path, `movePtr` bakes the address of
`wellKnownSymbols` directly into the instruction stream. In AOT mode,
`emitPatchableMovImm` emits a sentinel instead, and registers a
`RuntimePatch` to fix it up at load time. The resulting AOT assembly
(comments mine):

```asm
movzx      ecx, byte ptr [r14 + 1]
# sentinel value, to be patched at load time.
mov        rbx, 0xdeadbeef
mov        rcx, qword ptr [rbx + rcx*8]
mov        r11, rcx
shr        r11, 47
cmp        r11, r11
je         .valid_symbol
int3

.valid_symbol:
mov        r11, 0xfffb800000000000
or         rcx, r11
push       rcx
add        r14, 2
movzx      ecx, byte ptr [r14]
# perform the computed jump to the next opcode in $rcx.
lea        rbx, [rip + dispatch_table]
jmp        qword ptr [rbx + rcx*8]
```

The `0xdeadbeef` sentinel sits in the `mov rbx, 0xdeadbeef`
instruction, waiting to be patched at load time. The tail of the
handler illustrates threaded dispatch: we index into the dispatch
table and perform a computed jump directly to the next opcode handler,
avoiding a return to a central dispatch loop.


## Remaining Issues

The biggest glaring problem with this infrastructure is that it is not
automatically derived. If the interpreter maintainer decides to change
code generation, they also need to keep in mind registering patches
for AOT mode. This imposes more cognitive load on a complex project
which already demands plenty. We could improve the dump-time
verification. Another idea is to introduce some static analysis tool
to automate the detection of leaks. I am open to feedback!

## Conclusion

Thank you for reading. Stay tuned for part 2, where we explore
supplying an AOT Inline Cache stub corpus to make our AOT mode even
faster!

---

<small>

1. For architectures like x86-32 where free registers are not an
abundant resource, we need to fall back to loading from the frame each time.
Currently x64 is the only architecture the prototype works for.

2. For IC stubs, as we will talk about in part 2, we put a `JSZone*`
pointer into the `ICStub` data class. [Chase]() worked on this in
parallel, and we think we can consolidate to using the `JSZone*` from
the BaselineFrame instead.

3. Specifically, non-zero 64-bit immediates whose upper 17 bits are
all zero, which is the canonical form for user-space addresses on
x86-64. Large constants like NaN-boxing tags (`0x7ffffff00000`) are
whitelisted since they are not pointers.

</small>


